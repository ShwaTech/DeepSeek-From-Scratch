{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwmV1ALCFWu_"
      },
      "source": [
        "## HIDING FUTURE WORDS WITH CAUSAL ATTENTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXCnEbSjFWvA"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Let's work with the attention scores and weights from the previous section to code the causal attention mechanism.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_YxkQ2EFWvA"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "In the first step illustrated in Figure 3.20, we compute the attention weights using the\n",
        "softmax function as we have done in previous sections:    \n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap98DzXvFWvA"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "Reuse the query and key weight matrices of the SelfAttention_v2 object from the previous section for\n",
        "convenience\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reRcGuL4FWvA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention_v2(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.T\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec\n",
        "\n",
        "d_in = 3\n",
        "d_out = 6\n",
        "\n",
        "sa_v2 = SelfAttention_v2(d_in, d_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xn5z35wJFWvA",
        "outputId": "b1c24f8f-1607-41bf-d292-60113ec70e32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.1744, 0.1761, 0.1742, 0.1606, 0.1355, 0.1791],\n",
            "        [0.1674, 0.1822, 0.1796, 0.1595, 0.1253, 0.1860],\n",
            "        [0.1675, 0.1822, 0.1796, 0.1594, 0.1259, 0.1855],\n",
            "        [0.1660, 0.1753, 0.1739, 0.1638, 0.1425, 0.1785],\n",
            "        [0.1693, 0.1769, 0.1758, 0.1596, 0.1474, 0.1708],\n",
            "        [0.1651, 0.1763, 0.1744, 0.1646, 0.1353, 0.1843]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "queries = sa_v2.W_query(inputs) # A\n",
        "keys = sa_v2.W_key(inputs)\n",
        "\n",
        "attn_scores = queries @ keys.T\n",
        "\n",
        "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
        "\n",
        "print(attn_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KGiMzsiFWvA"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "We can now use PyTorch's tril function to create a mask\n",
        "where the values above the diagonal are zero:\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKQJ3U6hFWvA",
        "outputId": "a1c8960f-c593-4d7d-e8b4-a9ff6e463899"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1.]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "context_length = attn_scores.shape[0]\n",
        "\n",
        "torch.ones(context_length, context_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIaQc4CcFWvA",
        "outputId": "2aa6fe33-7ca3-40cc-fb97-61535d2744f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "context_length = attn_scores.shape[0]\n",
        "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
        "print(mask_simple)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIg4NPVuFWvA"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Now, we can multiply this mask with the attention weights to zero out the values above the\n",
        "diagonal:\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFPho7FqFWvA",
        "outputId": "8fdee93e-ff86-4650-b1f2-f97b26fd898e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.1744, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1674, 0.1822, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1675, 0.1822, 0.1796, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1660, 0.1753, 0.1739, 0.1638, 0.0000, 0.0000],\n",
            "        [0.1693, 0.1769, 0.1758, 0.1596, 0.1474, 0.0000],\n",
            "        [0.1651, 0.1763, 0.1744, 0.1646, 0.1353, 0.1843]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "masked_simple = attn_weights * mask_simple\n",
        "print(masked_simple)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZtADO1xFWvA"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "As we can see, the elements above the diagonal are successfully zeroed out\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPHnj5HrFWvA"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "The third step is to renormalize the attention weights to sum up to 1 again in\n",
        "each row.\n",
        "\n",
        "We can achieve this by dividing each element in each row by the sum in each\n",
        "row:\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yc9nscSJFWvA",
        "outputId": "7e3c0f33-2e90-46e2-fcc7-8043d2ce07f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4788, 0.5212, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3165, 0.3442, 0.3393, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2445, 0.2582, 0.2561, 0.2412, 0.0000, 0.0000],\n",
            "        [0.2042, 0.2134, 0.2121, 0.1925, 0.1778, 0.0000],\n",
            "        [0.1651, 0.1763, 0.1744, 0.1646, 0.1353, 0.1843]],\n",
            "       grad_fn=<DivBackward0>)\n"
          ]
        }
      ],
      "source": [
        "row_sums = masked_simple.sum(dim=1, keepdim=True)\n",
        "masked_simple_norm = masked_simple / row_sums\n",
        "print(masked_simple_norm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "365HQ314FWvA"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "The result is an attention weight matrix where the attention weights above the diagonal are\n",
        "zeroed out and where the rows sum to 1.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxVvzdIqFWvB"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "While we could be technically done with implementing causal attention at this point, we can\n",
        "take advantage of a mathematical property of the softmax function.\n",
        "\n",
        "We can implement the computation of the masked attention weights more efficiently in fewer steps.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEWVC6QBFWvB"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "The softmax function converts its inputs into a probability distribution.\n",
        "\n",
        "When negative\n",
        "infinity values (-∞) are present in a row, the softmax function treats them as zero\n",
        "probability.\n",
        "\n",
        "(Mathematically, this is because e\n",
        "-∞ approaches 0.)\n",
        "\n",
        "\n",
        "We can implement this more efficient masking \"trick\" by creating a mask with 1's above\n",
        "the diagonal and then replacing these 1's with negative infinity (-inf) values:\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIUF48uUFWvB",
        "outputId": "f21ff4bb-dcc7-4334-f640-1381931f6c47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.5470,  0.5700,  0.5444,  0.3443, -0.0708,  0.6124],\n",
            "        [ 0.6617,  0.8694,  0.8335,  0.5434, -0.0479,  0.9201],\n",
            "        [ 0.6619,  0.8674,  0.8321,  0.5405, -0.0385,  0.9116],\n",
            "        [ 0.3279,  0.4611,  0.4410,  0.2950, -0.0456,  0.5048],\n",
            "        [ 0.4803,  0.5879,  0.5729,  0.3358,  0.1414,  0.5021],\n",
            "        [ 0.3512,  0.5124,  0.4854,  0.3431, -0.1375,  0.6205]],\n",
            "       grad_fn=<MmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(attn_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7-NCgP5FWvB",
        "outputId": "7f98442d-164f-46f9-edcd-935cca21aff5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1., 1., 1.],\n",
              "        [0., 1., 1., 1., 1., 1.],\n",
              "        [0., 0., 1., 1., 1., 1.],\n",
              "        [0., 0., 0., 1., 1., 1.],\n",
              "        [0., 0., 0., 0., 1., 1.],\n",
              "        [0., 0., 0., 0., 0., 1.]])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.triu(torch.ones(context_length, context_length))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8geXBc5FWvB",
        "outputId": "398594ae-61ff-46ee-c296-043cfc8ab194"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., 1., 1., 1., 1., 1.],\n",
            "        [0., 0., 1., 1., 1., 1.],\n",
            "        [0., 0., 0., 1., 1., 1.],\n",
            "        [0., 0., 0., 0., 1., 1.],\n",
            "        [0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "print(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6TumoY-FWvB",
        "outputId": "8d4fa78c-c1d8-4a5e-cab6-17ef1273e5af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.5470,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "        [ 0.6617,  0.8694,    -inf,    -inf,    -inf,    -inf],\n",
            "        [ 0.6619,  0.8674,  0.8321,    -inf,    -inf,    -inf],\n",
            "        [ 0.3279,  0.4611,  0.4410,  0.2950,    -inf,    -inf],\n",
            "        [ 0.4803,  0.5879,  0.5729,  0.3358,  0.1414,    -inf],\n",
            "        [ 0.3512,  0.5124,  0.4854,  0.3431, -0.1375,  0.6205]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n"
          ]
        }
      ],
      "source": [
        "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
        "print(masked)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPfT1RiXFWvB"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Now, all we need to do is apply the softmax function to these masked results, and we are\n",
        "done.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2vKKoe5FWvB",
        "outputId": "a9939f2f-1a39-4035-f1b5-e06bb7341797"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4788, 0.5212, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3165, 0.3442, 0.3393, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2445, 0.2582, 0.2561, 0.2412, 0.0000, 0.0000],\n",
            "        [0.2042, 0.2134, 0.2121, 0.1925, 0.1778, 0.0000],\n",
            "        [0.1651, 0.1763, 0.1744, 0.1646, 0.1353, 0.1843]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
        "print(attn_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umUE9lo5FWvB"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "As we can see based on the output, the values in each row sum to 1, and no further\n",
        "normalization is necessary.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76A0sHH9FWvB"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "Masking in Transformers sets scores for future tokens to a large negative value, making their influence in the softmax calculation effectively zero.\n",
        "\n",
        "The softmax function then recalculates attention weights only among the unmasked tokens.\n",
        "\n",
        "This process ensures no information leakage from masked tokens, focusing the model solely on the intended data.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h7NxJNiFWvB"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "We could now use the modified attention weights to compute the context vectors via\n",
        "context_vec = attn_weights @ values.\n",
        "\n",
        "However, in the next section,\n",
        "we first cover another minor tweak to the causal attention mechanism that is useful for\n",
        "reducing overfitting when training LLMs.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7nJAU5cFWvB"
      },
      "source": [
        "### MASKING ADDITIONAL ATTENTION WEIGHTS WITH DROPOUT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_4rzKZuFWvB"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "In the following code example, we use a dropout rate of 50%, which means masking out\n",
        "half of the attention weights.\n",
        "\n",
        "When we train the GPT model in later chapters, we will use a\n",
        "lower dropout rate, such as 0.1 or 0.2.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWXPP3BZFWvB"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "In the following code, we apply PyTorch's dropout implementation first to a 6×6 tensor\n",
        "consisting of ones for illustration purposes:\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7d-MHuoFWvB",
        "outputId": "193de4a0-b4ba-4b8e-e66a-8e69172773db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "example = torch.ones(6, 6) # B\n",
        "print(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ksj7hw5gFWvC",
        "outputId": "330a9fb2-426c-41f9-ec20-21c02288a67f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[2., 2., 2., 2., 2., 2.],\n",
            "        [0., 2., 0., 0., 0., 0.],\n",
            "        [0., 0., 2., 0., 2., 0.],\n",
            "        [2., 2., 0., 0., 0., 2.],\n",
            "        [2., 0., 0., 0., 0., 2.],\n",
            "        [0., 2., 0., 0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "dropout = torch.nn.Dropout(0.5) # A\n",
        "example = torch.ones(6, 6) # B\n",
        "print(dropout(example))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLPuulAJFWvC"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "When applying dropout to an attention weight matrix with a rate of 50%, half of the\n",
        "elements in the matrix are randomly set to zero.\n",
        "\n",
        "To compensate for the reduction in active\n",
        "elements, the values of the remaining elements in the matrix are scaled up by a factor of\n",
        "1/0.5 =2.\n",
        "\n",
        "This scaling is crucial to maintain the overall balance of the attention weights,\n",
        "ensuring that the average influence of the attention mechanism remains consistent during\n",
        "both the training and inference phases.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssyQVvjSFWvC"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Now, let's apply dropout to the attention weight matrix itself:\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFLINDb3FWvC",
        "outputId": "631698f2-fae5-42a6-c297-7b2bf090aee2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 1.0424, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.6786, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4890, 0.5164, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4084, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.3527, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "print(dropout(attn_weights))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qEeYOGTFWvC"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "As we can see above, the resulting attention weight matrix now has additional elements zeroed out and the\n",
        "remaining ones rescaled.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhQQQ2Z7FWvC"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "Having gained an understanding of causal attention and dropout masking, we will\n",
        "develop a concise Python class in the following section.\n",
        "\n",
        "This class is designed to facilitate\n",
        "the efficient application of these two techniques.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4QIqG8NFWvC"
      },
      "source": [
        "### IMPLEMENTING A COMPACT CAUSAL ATTENTION CLASS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxtlawsOFWvC"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "In this section, we will now incorporate the causal attention and dropout modifications into\n",
        "the SelfAttention Python class we developed in section 3.4.\n",
        "\n",
        "This class will then serve as a\n",
        "template for developing multi-head attention in the upcoming section.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea08otyLFWvC"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Before we begin, one more thing is to ensure that the code can handle batches\n",
        "consisting of more than one input.\n",
        "\n",
        "This will ensure that the CausalAttention class supports the batch\n",
        "outputs produced by the data loader we implemented earlier.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efqYP8x6FWvC"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "For simplicity, to simulate such batch inputs, we duplicate the input text example:\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V31p-K1CFWvC"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        " 2 inputs with 6 tokens each, and each token has embedding dimension 3\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Z8ituoHFWvC",
        "outputId": "86c8b807-fdeb-49c1-9ec2-e13f6dd5c910"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 6, 3])\n"
          ]
        }
      ],
      "source": [
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")\n",
        "\n",
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "print(batch.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO-xLxdsFWvC"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "This results in a 3D tensor consisting of 2 input texts with 6 tokens each, where each token\n",
        "is a 3-dimensional embedding vector.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYU3JdZMFWvC"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "The following CausalAttention class is similar to the SelfAttention class we\n",
        "implemented earlier, except that we now added the dropout and causal mask components\n",
        "as highlighted in the following code.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uW94U93FWvC"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "Step 1: Compared to the previous SelfAttention_v1 class, we added a dropout layer.\n",
        "    \n",
        "Step 2: The register_buffer call is also a new addition (more information is provided in the following text).\n",
        "\n",
        "Step 3:  We transpose dimensions 1 and 2, keeping the batch dimension at the first position (0).\n",
        "\n",
        "Step 4: In PyTorch, operations with a trailing underscore are performed in-place, avoiding unnecessary memory\n",
        "copies\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "PXzvbuCUFWvC"
      },
      "outputs": [],
      "source": [
        "class CausalAttention(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_in,\n",
        "        d_out,\n",
        "        context_length,\n",
        "        dropout,\n",
        "        qkv_bias=False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.d_out = d_out\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.dropout = nn.Dropout(dropout) # New\n",
        "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape # New batch dimension b\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n",
        "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
        "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf\n",
        "        )  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        "        )\n",
        "        attn_weights = self.dropout(attn_weights) # New\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "es97tSQvFWvD"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "The use of register_buffer in\n",
        "PyTorch is not strictly necessary for all use cases but offers several advantages here.\n",
        "\n",
        "For\n",
        "instance, when we use the CausalAttention class in our LLM, buffers are automatically\n",
        "moved to the appropriate device (CPU or GPU) along with our model, which will be relevant\n",
        "when training the LLM in future chapters.\n",
        "\n",
        "This means we don't need to manually ensure\n",
        "these tensors are on the same device as your model parameters, avoiding device mismatch\n",
        "errors.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUqrgYbuFWvD"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "We can use the CausalAttention class as follows, similar to SelfAttention previously:\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aCHR9NzFWvD",
        "outputId": "20998451-3730-45ae-cb5a-1bab5886c3b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n"
          ]
        }
      ],
      "source": [
        "print(d_in)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dslY6H0BFWvD",
        "outputId": "c31de78a-d5b7-4916-e6e8-7f2017857000"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n"
          ]
        }
      ],
      "source": [
        "print(d_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exCqc4LeFWvD",
        "outputId": "2bdf78ed-cd92-4b68-c5b0-0cacc287ba33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "context_vecs.shape: torch.Size([2, 6, 6])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "context_length = batch.shape[1]\n",
        "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
        "context_vecs = ca(batch)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6--Bjf_FWvD",
        "outputId": "e8933934-8da6-4f67-b38c-2e40bb99bebd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[-0.5967, -0.2754,  0.0189,  0.0752,  0.4566,  0.2729],\n",
            "         [-0.6362, -0.2762,  0.2052,  0.0361,  0.5823,  0.3018],\n",
            "         [-0.6500, -0.2736,  0.2695,  0.0220,  0.6279,  0.3108],\n",
            "         [-0.5709, -0.2437,  0.2775,  0.0111,  0.5678,  0.2779],\n",
            "         [-0.5475, -0.1987,  0.2400,  0.0087,  0.5555,  0.2520],\n",
            "         [-0.5198, -0.2079,  0.2659,  0.0042,  0.5322,  0.2494]],\n",
            "\n",
            "        [[-0.5967, -0.2754,  0.0189,  0.0752,  0.4566,  0.2729],\n",
            "         [-0.6362, -0.2762,  0.2052,  0.0361,  0.5823,  0.3018],\n",
            "         [-0.6500, -0.2736,  0.2695,  0.0220,  0.6279,  0.3108],\n",
            "         [-0.5709, -0.2437,  0.2775,  0.0111,  0.5678,  0.2779],\n",
            "         [-0.5475, -0.1987,  0.2400,  0.0087,  0.5555,  0.2520],\n",
            "         [-0.5198, -0.2079,  0.2659,  0.0042,  0.5322,  0.2494]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(context_vecs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jz_gR3mXF2u_",
        "outputId": "3f910650-f7cb-480b-8f08-dec1e0cb8d33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention Weights Before Dropout:\n",
            " tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.5321, 0.4679, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.3616, 0.3185, 0.3198, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2627, 0.2408, 0.2413, 0.2552, 0.0000, 0.0000],\n",
            "         [0.2046, 0.1921, 0.1926, 0.2020, 0.2088, 0.0000],\n",
            "         [0.1759, 0.1582, 0.1586, 0.1690, 0.1747, 0.1637]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.5321, 0.4679, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.3616, 0.3185, 0.3198, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2627, 0.2408, 0.2413, 0.2552, 0.0000, 0.0000],\n",
            "         [0.2046, 0.1921, 0.1926, 0.2020, 0.2088, 0.0000],\n",
            "         [0.1759, 0.1582, 0.1586, 0.1690, 0.1747, 0.1637]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Attention Weights After Dropout:\n",
            " tensor([[[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.9359, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.7232, 0.6371, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.5255, 0.0000, 0.0000, 0.5104, 0.0000, 0.0000],\n",
            "         [0.4092, 0.3841, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.3517, 0.0000, 0.3172, 0.0000, 0.0000, 0.3273]],\n",
            "\n",
            "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.9359, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.7232, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.5255, 0.4815, 0.4827, 0.5104, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.4040, 0.4176, 0.0000],\n",
            "         [0.3517, 0.0000, 0.3172, 0.0000, 0.3494, 0.3273]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 6])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CausalAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.d_out = d_out\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.dropout = nn.Dropout(dropout) # New\n",
        "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape # New batch dimension b\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n",
        "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
        "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        "        )\n",
        "\n",
        "        print(\"Attention Weights Before Dropout:\\n\", attn_weights)\n",
        "\n",
        "        attn_weights = self.dropout(attn_weights) # New\n",
        "\n",
        "        print(\"Attention Weights After Dropout:\\n\", attn_weights)\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec\n",
        "\n",
        "torch.manual_seed(123)\n",
        "context_length = batch.shape[1]\n",
        "ca = CausalAttention(d_in, d_out, context_length, 0.5)\n",
        "context_vecs = ca(batch)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8-dmlqYFWvD"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "As we can see, the resulting context vector is a 3D tensor where each token is now represented by a 2D\n",
        "embedding:\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU1CBDdWFWvD"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "In the next section, we will expand on this concept\n",
        "and implement a multi-head attention module, that implements several of such causal\n",
        "attention mechanisms in parallel.\n",
        "\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
